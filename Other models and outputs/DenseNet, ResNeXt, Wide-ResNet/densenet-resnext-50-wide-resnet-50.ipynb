{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10335757,"sourceType":"datasetVersion","datasetId":6399958}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":1217.281589,"end_time":"2025-04-04T15:59:36.276106","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-04T15:39:18.994517","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\n!pip install thop","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-04-13T18:18:26.910501Z","iopub.execute_input":"2025-04-13T18:18:26.911170Z","iopub.status.idle":"2025-04-13T18:18:32.647212Z","shell.execute_reply.started":"2025-04-13T18:18:26.911135Z","shell.execute_reply":"2025-04-13T18:18:32.646350Z"},"papermill":{"duration":12.44928,"end_time":"2025-04-04T15:39:34.043387","exception":false,"start_time":"2025-04-04T15:39:21.594107","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: thop in /usr/local/lib/python3.11/dist-packages (0.1.1.post2209072238)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from thop) (2.5.1+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->thop) (3.0.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms, datasets\nfrom torch.utils.data import DataLoader, random_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.preprocessing import label_binarize\nimport os\nimport copy\nimport json\nimport gc  # For memory management\n\n# Helper function for formatting large numbers\ndef format_units(num):\n    \"\"\"Format large numbers with units (K, M, G, etc.)\"\"\"\n    magnitude = 0\n    while abs(num) >= 1000:\n        magnitude += 1\n        num /= 1000.0\n    return f\"{num:.2f} {['', 'K', 'M', 'G', 'T', 'P'][magnitude]}\"\n\n# Device Configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Clean up CUDA memory\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# Parameters\nbatch_size = 16\nstandard_img_size = 224  # Standard size for most models (DenseNet, ResNeXt, Wide ResNet)\nnum_epochs = 50\nlearning_rate = 1e-4\nsplit_ratio = [0.7, 0.15, 0.15]  # 70% training, 15% validation, 15% test\n\n# Dataset Directory\ndataset_dir = \"/kaggle/input/drone-usat/DIAT-uSAT_dataset\"  # Directory for DIAT-uSAT dataset\n\n# Data Transformations\ntransform = transforms.Compose([\n    transforms.Resize((standard_img_size, standard_img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Function to create data loaders with proper dataset splits and consistent indices\ndef create_data_loaders(full_dataset):\n    # Get a generator with fixed seed for consistent splits\n    generator = torch.Generator().manual_seed(42)\n    \n    # Split into Train, Validation, and Test\n    train_size = int(split_ratio[0] * len(full_dataset))\n    val_size = int(split_ratio[1] * len(full_dataset))\n    test_size = len(full_dataset) - train_size - val_size\n    \n    train_dataset, val_dataset, test_dataset = random_split(\n        full_dataset, \n        [train_size, val_size, test_size],\n        generator=generator\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset\n\n# Load Full Dataset\ntry:\n    full_dataset = datasets.ImageFolder(root=dataset_dir, transform=transform)\n    print(f\"Successfully loaded dataset with {len(full_dataset)} images\")\nexcept Exception as e:\n    print(f\"Error loading dataset: {str(e)}\")\n    print(\"Please verify the dataset path and format\")\n    raise\n\n# Create data loaders\ntrain_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = create_data_loaders(full_dataset)\n\n# Print dataset information\nprint(f\"Total number of samples: {len(full_dataset)}\")\nclass_to_idx = full_dataset.class_to_idx\nprint(\"Class to index mapping:\", class_to_idx)\nfor class_name, idx in class_to_idx.items():\n    class_samples = len([x for x, y in full_dataset.samples if y == idx])\n    print(f\"Class {class_name}: {class_samples} samples\")\n\n# Number of Classes\nnum_classes = len(full_dataset.classes)\nclass_names = full_dataset.classes\nprint(f\"Number of classes: {num_classes}\")\nprint(f\"Class Names: {class_names}\")\n\n# Custom model summary function that works with any architecture\ndef get_model_summary(model, input_size=(1, 3, 224, 224)):\n    \"\"\"\n    A custom function to generate model summary that works with all architectures.\n    \n    Args:\n        model: PyTorch model\n        input_size: Input tensor size (batch_size, channels, height, width)\n        \n    Returns:\n        model_info: String with model information\n    \"\"\"\n    # Make sure the model is on the same device for summary\n    device = next(model.parameters()).device\n    model_info = f\"Model device: {device}\\n\"\n    \n    # Count parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    model_info += f\"Total Parameters: {total_params:,}\\n\"\n    model_info += f\"Trainable Parameters: {trainable_params:,}\\n\\n\"\n    \n    # Try to get layer info without causing errors\n    try:\n        # Create a dummy input on the same device as the model\n        x = torch.rand(input_size).to(device)\n        \n        # Record important layers\n        model_info += \"Key Layers:\\n\"\n        model_info += \"-\" * 80 + \"\\n\"\n        model_info += f\"{'Layer Type':<25} {'Parameters':<15}\\n\"\n        model_info += \"-\" * 80 + \"\\n\"\n        \n        # Get important modules\n        modules = []\n        for name, module in model.named_modules():\n            if isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm2d, nn.MaxPool2d, \n                                  nn.AdaptiveAvgPool2d)) and not any(name.startswith(n + \".\") \n                                                                   for n in [m[0] for m in modules if m[0]]):\n                modules.append((name, module))\n        \n        # Display layer info\n        for name, module in modules:\n            params = sum(p.numel() for p in module.parameters())\n            module_type = module.__class__.__name__\n            model_info += f\"{module_type:<25} {params:,}\\n\"\n    \n    except Exception as e:\n        model_info += f\"Could not generate detailed layer information due to: {str(e)}\\n\"\n        \n    # Add model structure summary (but not the full structure to save memory)\n    model_info += \"\\nModel Architecture Type:\\n\"\n    model_info += str(model.__class__.__name__)\n    \n    return model_info\n        \n# Define a function to train and evaluate a model\ndef train_and_evaluate_model(model_name, model, train_loader, val_loader, test_loader, test_dataset, full_dataset):\n    print(f\"\\n{'='*50}\")\n    print(f\"Training and Evaluating {model_name}\")\n    print(f\"{'='*50}\")\n    \n    # Create a results directory for this model\n    results_dir = os.path.join(\"results\", model_name)\n    os.makedirs(results_dir, exist_ok=True)\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Print model summary using our custom function\n    print(f\"\\n{model_name} Summary:\")\n    print(get_model_summary(model, input_size=(1, 3, standard_img_size, standard_img_size)))\n    \n    # Loss and Optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n    \n    # Training Loop\n    best_val_acc = 0.0\n    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n    early_stop_counter = 0\n    early_stop_patience = 5\n    best_model_weights = None\n    \n    print(f\"\\nStarting training {model_name}...\")\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        \n        for i, (images, labels) in enumerate(train_loader):\n            images, labels = images.to(device), labels.to(device)\n            \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n            # Free up memory\n            del images, labels, outputs\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n        avg_train_loss = running_loss / len(train_loader)\n        history['train_loss'].append(avg_train_loss)\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n                \n                # Free up memory\n                del images, labels, outputs, predicted\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n        \n        avg_val_loss = val_loss / len(val_loader)\n        val_accuracy = correct / total\n        history['val_loss'].append(avg_val_loss)\n        history['val_acc'].append(val_accuracy)\n        \n        # Update learning rate scheduler\n        scheduler.step(avg_val_loss)\n        \n        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n        \n        # Save the best model\n        if val_accuracy > best_val_acc:\n            best_val_acc = val_accuracy\n            best_model_weights = copy.deepcopy(model.state_dict())\n            torch.save(model.state_dict(), os.path.join(results_dir, f\"{model_name.lower().replace('-', '_')}_best.pth\"))\n            print(f\"Model saved with validation accuracy: {val_accuracy:.4f}\")\n            early_stop_counter = 0\n        else:\n            early_stop_counter += 1\n        \n        # Early stopping\n        if early_stop_counter >= early_stop_patience:\n            print(f\"Early stopping triggered after {epoch+1} epochs\")\n            break\n            \n        # Clean up memory\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    \n    # Plot training history\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Validation Loss')\n    plt.legend()\n    plt.title(f'{model_name} - Loss Over Epochs')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history['val_acc'], label='Validation Accuracy')\n    plt.legend()\n    plt.title(f'{model_name} - Accuracy Over Epochs')\n    plt.savefig(os.path.join(results_dir, f'{model_name.lower().replace(\"-\", \"_\")}_training_history.png'))\n    plt.close()\n    \n    # Load the best model for evaluation\n    model.load_state_dict(best_model_weights)\n    model.eval()\n    \n    # Testing the Model\n    print(f\"\\nEvaluating {model_name} on test set...\")\n    y_true = []\n    y_pred = []\n    y_scores = []\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs.data, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n            y_scores.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n            \n            # Free up memory\n            del images, labels, outputs, preds\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n    \n    # Classification Report\n    cls_report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n    print(f\"\\n{model_name} Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=class_names))\n    \n    # Confusion Matrix\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel(\"Predicted Labels\")\n    plt.ylabel(\"True Labels\")\n    plt.title(f\"{model_name} Confusion Matrix\")\n    plt.savefig(os.path.join(results_dir, f\"{model_name.lower().replace('-', '_')}_confusion_matrix.png\"))\n    plt.close()\n    \n    # ROC Curve (for multi-class classification)\n    roc_auc = None\n    if num_classes > 2:\n        try:\n            y_true_bin = label_binarize(y_true, classes=np.arange(num_classes))\n            y_scores_array = np.array(y_scores)\n            \n            # Calculate ROC AUC\n            roc_auc = roc_auc_score(y_true_bin, y_scores_array, multi_class=\"ovr\")\n            print(f\"\\n{model_name} Multi-class ROC AUC Score: {roc_auc:.4f}\")\n        \n            # Plot ROC curves\n            plt.figure(figsize=(10, 8))\n            for i in range(num_classes):\n                fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_scores_array[:, i])\n                auc_score = roc_auc_score(y_true_bin[:, i], y_scores_array[:, i])\n                plt.plot(fpr, tpr, label=f\"Class {class_names[i]} (AUC = {auc_score:.2f})\")\n        \n            plt.plot([0, 1], [0, 1], \"k--\")\n            plt.xlabel(\"False Positive Rate\")\n            plt.ylabel(\"True Positive Rate\")\n            plt.title(f\"{model_name} ROC Curve\")\n            plt.legend()\n            plt.savefig(os.path.join(results_dir, f\"{model_name.lower().replace('-', '_')}_roc_curve.png\"))\n            plt.close()\n        except Exception as e:\n            print(f\"Error generating ROC curve: {str(e)}\")\n    \n    # Inference Time Calculation\n    sample_input = torch.randn(1, 3, standard_img_size, standard_img_size).to(device)\n    \n    # Warm-up runs\n    with torch.no_grad():\n        for _ in range(10): \n            _ = model(sample_input)\n            \n    # Actual timing runs\n    num_samples = 100\n    start_time = time.time()\n    with torch.no_grad():\n        for _ in range(num_samples):\n            _ = model(sample_input)\n    inference_time = (time.time() - start_time) / num_samples\n    \n    # Number of Parameters\n    num_params = sum(p.numel() for p in model.parameters())\n    \n    # FLOPs & MACs Calculation\n    try:\n        from thop import profile\n        flops, macs = profile(model, inputs=(sample_input,), verbose=False)\n    except Exception as e:\n        print(f\"Error calculating FLOPs and MACs: {str(e)}\")\n        flops, macs = 0, 0\n    \n    # Convert inference time to milliseconds\n    inference_time_ms = inference_time * 1000\n    \n    print(f\"\\n{model_name} Total Number of Parameters: {num_params:,}\")\n    print(f\"{model_name} Average Inference Time per Sample: {inference_time_ms:.3f} ms\")\n    print(f\"{model_name} FLOPs: {flops:,} ({format_units(flops)})\")\n    print(f\"{model_name} MACs: {macs:,} ({format_units(macs)})\\n\")\n    \n    # Per-class accuracy\n    class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n    class_acc_dict = {}\n    for i, acc in enumerate(class_accuracy):\n        print(f\"✅ {model_name} Accuracy for class '{class_names[i]}': {acc:.2%}\")\n        class_acc_dict[class_names[i]] = float(acc)\n    \n    # Calculate and display test accuracy with 3 decimal places\n    test_correct = sum([1 for i, j in zip(y_true, y_pred) if i == j])\n    test_total = len(y_true)\n    test_accuracy = test_correct / test_total\n    print(f\"\\n✅ {model_name} Test Set Accuracy: {test_accuracy:.3f}\")\n    \n    # Calculate and display model size in MB\n    # Each parameter is typically stored as a 32-bit float (4 bytes)\n    model_size_bytes = num_params * 4\n    model_size_mb = model_size_bytes / (1024 * 1024)\n    print(f\"📊 {model_name} Model Size: {model_size_mb:.2f} MB\")\n    \n    # Model characteristics based on model name\n    characteristics = []\n    if model_name == \"DenseNet-121\":\n        characteristics = [\n            \"Dense connectivity pattern with direct connections from any layer to all subsequent layers\",\n            \"Excellent feature reuse through dense connections\",\n            \"Requires fewer parameters due to feature reuse\",\n            \"Good performance with reduced overfitting\",\n            \"Efficient gradient flow during training\"\n        ]\n    elif model_name == \"ResNeXt-50\":\n        characteristics = [\n            \"Extension of ResNet that aggregates residual transformations\",\n            \"Uses split-transform-merge strategy with grouped convolutions\",\n            \"Better performance than ResNet with similar complexity\",\n            \"Higher accuracy-to-computation ratio than many models\",\n            \"Cardinality dimension provides a more effective way to adjust model capacity\"\n        ]\n    elif model_name == \"Wide ResNet-50\":\n        characteristics = [\n            \"Modification of ResNet with increased width (channel count) and reduced depth\",\n            \"Wider networks often achieve better performance than deeper ones\",\n            \"More efficient to train than very deep networks\",\n            \"Better feature extraction capability with wider channels\",\n            \"Strong classification performance with reasonable computational cost\"\n        ]\n    \n    # Print characteristics\n    print(f\"\\nKey characteristics of {model_name}:\")\n    for char in characteristics:\n        print(f\"- {char}\")\n    \n    # Save all metrics to a JSON file\n    metrics = {\n        \"model_name\": model_name,\n        \"test_accuracy\": float(test_accuracy),\n        \"inference_time_ms\": float(inference_time_ms),\n        \"model_size_mb\": float(model_size_mb),\n        \"parameters\": int(num_params),\n        \"flops\": int(flops),\n        \"macs\": int(macs),\n        \"roc_auc_score\": float(roc_auc) if roc_auc is not None else None,\n        \"per_class_accuracy\": class_acc_dict,\n        \"classification_report\": cls_report,\n        \"characteristics\": characteristics\n    }\n    \n    with open(os.path.join(results_dir, f\"{model_name.lower().replace('-', '_')}_metrics.json\"), \"w\") as f:\n        json.dump(metrics, f, indent=4)\n    \n    print(f\"\\nMetrics saved to {os.path.join(results_dir, model_name.lower().replace('-', '_') + '_metrics.json')}\")\n    \n    # Clean up memory\n    del best_model_weights\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        \n    return model\n\n# Train models sequentially with improved memory handling\ntry:\n    # Create the results directory\n    if not os.path.exists(\"results\"):\n        os.makedirs(\"results\")\n        \n    # 1. DenseNet-121\n    print(\"\\n\\n\" + \"=\"*80)\n    print(\"TRAINING DENSENET-121\")\n    print(\"=\"*80)\n    densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n    densenet.classifier = nn.Linear(densenet.classifier.in_features, num_classes)\n    train_and_evaluate_model('DenseNet-121', densenet, train_loader, val_loader, test_loader, test_dataset, full_dataset)\n    \n    # Clear memory before next model\n    del densenet\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # 2. ResNeXt-50\n    print(\"\\n\\n\" + \"=\"*80)\n    print(\"TRAINING RESNEXT-50\")\n    print(\"=\"*80)\n    resnext = models.resnext50_32x4d(weights=models.ResNeXt50_32X4D_Weights.IMAGENET1K_V1)\n    resnext.fc = nn.Linear(resnext.fc.in_features, num_classes)\n    train_and_evaluate_model('ResNeXt-50', resnext, train_loader, val_loader, test_loader, test_dataset, full_dataset)\n    \n    # Clear memory before next model\n    del resnext\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # 3. Wide ResNet-50\n    print(\"\\n\\n\" + \"=\"*80)\n    print(\"TRAINING WIDE RESNET-50\")\n    print(\"=\"*80)\n    wide_resnet = models.wide_resnet50_2(weights=models.Wide_ResNet50_2_Weights.IMAGENET1K_V1)\n    wide_resnet.fc = nn.Linear(wide_resnet.fc.in_features, num_classes)\n    train_and_evaluate_model('Wide ResNet-50', wide_resnet, train_loader, val_loader, test_loader, test_dataset, full_dataset)\n    \n    print(\"\\n\\nAll models have been trained and evaluated!\")\n    print(\"Results have been saved to individual folders in the 'results' directory\")\n    \nexcept Exception as e:\n    print(f\"An error occurred during execution: {str(e)}\")\n    import traceback\n    traceback.print_exc()","metadata":{"execution":{"iopub.status.busy":"2025-04-13T18:18:32.649108Z","iopub.execute_input":"2025-04-13T18:18:32.649758Z","iopub.status.idle":"2025-04-13T19:52:12.776809Z","shell.execute_reply.started":"2025-04-13T18:18:32.649736Z","shell.execute_reply":"2025-04-13T19:52:12.776032Z"},"papermill":{"duration":1199.538183,"end_time":"2025-04-04T15:59:33.583890","exception":false,"start_time":"2025-04-04T15:39:34.045707","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\nSuccessfully loaded dataset with 4849 images\nTotal number of samples: 4849\nClass to index mapping: {'3_long_blade_rotor': 0, '3_short_blade_rotor_1': 1, '3_short_blade_rotor_2': 2, 'Bird': 3, 'Bird+mini-helicopter_1': 4, 'Bird+mini-helicopter_2': 5, 'RC plane_1': 6, 'RC plane_2': 7, 'drone_1': 8, 'drone_2': 9}\nClass 3_long_blade_rotor: 799 samples\nClass 3_short_blade_rotor_1: 400 samples\nClass 3_short_blade_rotor_2: 400 samples\nClass Bird: 800 samples\nClass Bird+mini-helicopter_1: 415 samples\nClass Bird+mini-helicopter_2: 400 samples\nClass RC plane_1: 400 samples\nClass RC plane_2: 400 samples\nClass drone_1: 400 samples\nClass drone_2: 435 samples\nNumber of classes: 10\nClass Names: ['3_long_blade_rotor', '3_short_blade_rotor_1', '3_short_blade_rotor_2', 'Bird', 'Bird+mini-helicopter_1', 'Bird+mini-helicopter_2', 'RC plane_1', 'RC plane_2', 'drone_1', 'drone_2']\n\n\n================================================================================\nTRAINING DENSENET-121\n================================================================================\n\n==================================================\nTraining and Evaluating DenseNet-121\n==================================================\n\nDenseNet-121 Summary:\nModel device: cuda:0\nTotal Parameters: 6,964,106\nTrainable Parameters: 6,964,106\n\nKey Layers:\n--------------------------------------------------------------------------------\nLayer Type                Parameters     \n--------------------------------------------------------------------------------\nConv2d                    9,408\nBatchNorm2d               128\nMaxPool2d                 0\nBatchNorm2d               128\nConv2d                    8,192\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               192\nConv2d                    12,288\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               256\nConv2d                    16,384\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               320\nConv2d                    20,480\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               384\nConv2d                    24,576\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               448\nConv2d                    28,672\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               512\nConv2d                    32,768\nBatchNorm2d               256\nConv2d                    16,384\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               320\nConv2d                    20,480\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               384\nConv2d                    24,576\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               448\nConv2d                    28,672\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               512\nConv2d                    32,768\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               576\nConv2d                    36,864\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               640\nConv2d                    40,960\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               704\nConv2d                    45,056\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               768\nConv2d                    49,152\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               832\nConv2d                    53,248\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               896\nConv2d                    57,344\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               960\nConv2d                    61,440\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,024\nConv2d                    131,072\nBatchNorm2d               512\nConv2d                    32,768\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               576\nConv2d                    36,864\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               640\nConv2d                    40,960\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               704\nConv2d                    45,056\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               768\nConv2d                    49,152\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               832\nConv2d                    53,248\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               896\nConv2d                    57,344\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               960\nConv2d                    61,440\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,024\nConv2d                    65,536\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,088\nConv2d                    69,632\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,152\nConv2d                    73,728\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,216\nConv2d                    77,824\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,280\nConv2d                    81,920\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,344\nConv2d                    86,016\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,408\nConv2d                    90,112\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,472\nConv2d                    94,208\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,536\nConv2d                    98,304\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,600\nConv2d                    102,400\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,664\nConv2d                    106,496\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,728\nConv2d                    110,592\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,792\nConv2d                    114,688\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,856\nConv2d                    118,784\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,920\nConv2d                    122,880\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,984\nConv2d                    126,976\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               2,048\nConv2d                    524,288\nBatchNorm2d               1,024\nConv2d                    65,536\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,088\nConv2d                    69,632\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,152\nConv2d                    73,728\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,216\nConv2d                    77,824\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,280\nConv2d                    81,920\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,344\nConv2d                    86,016\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,408\nConv2d                    90,112\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,472\nConv2d                    94,208\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,536\nConv2d                    98,304\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,600\nConv2d                    102,400\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,664\nConv2d                    106,496\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,728\nConv2d                    110,592\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,792\nConv2d                    114,688\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,856\nConv2d                    118,784\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,920\nConv2d                    122,880\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               1,984\nConv2d                    126,976\nBatchNorm2d               256\nConv2d                    36,864\nBatchNorm2d               2,048\nLinear                    10,250\n\nModel Architecture Type:\nDenseNet\n\nStarting training DenseNet-121...\nEpoch [1/50], Train Loss: 0.6601, Val Loss: 0.2599, Val Accuracy: 0.9106\nModel saved with validation accuracy: 0.9106\nEpoch [2/50], Train Loss: 0.2376, Val Loss: 0.1531, Val Accuracy: 0.9477\nModel saved with validation accuracy: 0.9477\nEpoch [3/50], Train Loss: 0.1354, Val Loss: 0.1146, Val Accuracy: 0.9642\nModel saved with validation accuracy: 0.9642\nEpoch [4/50], Train Loss: 0.0922, Val Loss: 0.1352, Val Accuracy: 0.9491\nEpoch [5/50], Train Loss: 0.0520, Val Loss: 0.1018, Val Accuracy: 0.9684\nModel saved with validation accuracy: 0.9684\nEpoch [6/50], Train Loss: 0.0818, Val Loss: 0.1168, Val Accuracy: 0.9615\nEpoch [7/50], Train Loss: 0.0550, Val Loss: 0.1018, Val Accuracy: 0.9684\nEpoch [8/50], Train Loss: 0.0394, Val Loss: 0.0844, Val Accuracy: 0.9697\nModel saved with validation accuracy: 0.9697\nEpoch [9/50], Train Loss: 0.0377, Val Loss: 0.0710, Val Accuracy: 0.9711\nModel saved with validation accuracy: 0.9711\nEpoch [10/50], Train Loss: 0.0221, Val Loss: 0.0739, Val Accuracy: 0.9821\nModel saved with validation accuracy: 0.9821\nEpoch [11/50], Train Loss: 0.0268, Val Loss: 0.0835, Val Accuracy: 0.9780\nEpoch [12/50], Train Loss: 0.0305, Val Loss: 0.1209, Val Accuracy: 0.9656\nEpoch [13/50], Train Loss: 0.0251, Val Loss: 0.0587, Val Accuracy: 0.9849\nModel saved with validation accuracy: 0.9849\nEpoch [14/50], Train Loss: 0.0203, Val Loss: 0.0668, Val Accuracy: 0.9794\nEpoch [15/50], Train Loss: 0.0371, Val Loss: 0.0738, Val Accuracy: 0.9794\nEpoch [16/50], Train Loss: 0.0387, Val Loss: 0.1175, Val Accuracy: 0.9574\nEpoch [17/50], Train Loss: 0.0271, Val Loss: 0.0793, Val Accuracy: 0.9780\nEpoch [18/50], Train Loss: 0.0152, Val Loss: 0.0498, Val Accuracy: 0.9849\nEarly stopping triggered after 18 epochs\n\nEvaluating DenseNet-121 on test set...\n\nDenseNet-121 Classification Report:\n                        precision    recall  f1-score   support\n\n    3_long_blade_rotor       0.98      0.98      0.98       118\n 3_short_blade_rotor_1       0.92      0.99      0.95        71\n 3_short_blade_rotor_2       0.98      0.93      0.95        67\n                  Bird       1.00      1.00      1.00       107\nBird+mini-helicopter_1       0.93      0.93      0.93        57\nBird+mini-helicopter_2       0.94      0.92      0.93        50\n            RC plane_1       1.00      1.00      1.00        56\n            RC plane_2       1.00      1.00      1.00        66\n               drone_1       1.00      0.94      0.97        63\n               drone_2       0.95      1.00      0.97        73\n\n              accuracy                           0.97       728\n             macro avg       0.97      0.97      0.97       728\n          weighted avg       0.97      0.97      0.97       728\n\n\nDenseNet-121 Multi-class ROC AUC Score: 0.9997\n\nDenseNet-121 Total Number of Parameters: 6,964,106\nDenseNet-121 Average Inference Time per Sample: 15.000 ms\nDenseNet-121 FLOPs: 2,895,993,344.0 (2.90 G)\nDenseNet-121 MACs: 6,964,106.0 (6.96 M)\n\n✅ DenseNet-121 Accuracy for class '3_long_blade_rotor': 98.31%\n✅ DenseNet-121 Accuracy for class '3_short_blade_rotor_1': 98.59%\n✅ DenseNet-121 Accuracy for class '3_short_blade_rotor_2': 92.54%\n✅ DenseNet-121 Accuracy for class 'Bird': 100.00%\n✅ DenseNet-121 Accuracy for class 'Bird+mini-helicopter_1': 92.98%\n✅ DenseNet-121 Accuracy for class 'Bird+mini-helicopter_2': 92.00%\n✅ DenseNet-121 Accuracy for class 'RC plane_1': 100.00%\n✅ DenseNet-121 Accuracy for class 'RC plane_2': 100.00%\n✅ DenseNet-121 Accuracy for class 'drone_1': 93.65%\n✅ DenseNet-121 Accuracy for class 'drone_2': 100.00%\n\n✅ DenseNet-121 Test Set Accuracy: 0.973\n📊 DenseNet-121 Model Size: 26.57 MB\n\nKey characteristics of DenseNet-121:\n- Dense connectivity pattern with direct connections from any layer to all subsequent layers\n- Excellent feature reuse through dense connections\n- Requires fewer parameters due to feature reuse\n- Good performance with reduced overfitting\n- Efficient gradient flow during training\n\nMetrics saved to results/DenseNet-121/densenet_121_metrics.json\n\n\n================================================================================\nTRAINING RESNEXT-50\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n100%|██████████| 95.8M/95.8M [00:00<00:00, 207MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nTraining and Evaluating ResNeXt-50\n==================================================\n\nResNeXt-50 Summary:\nModel device: cuda:0\nTotal Parameters: 23,000,394\nTrainable Parameters: 23,000,394\n\nKey Layers:\n--------------------------------------------------------------------------------\nLayer Type                Parameters     \n--------------------------------------------------------------------------------\nConv2d                    9,408\nBatchNorm2d               128\nMaxPool2d                 0\nConv2d                    8,192\nBatchNorm2d               256\nConv2d                    4,608\nBatchNorm2d               256\nConv2d                    32,768\nBatchNorm2d               512\nConv2d                    16,384\nBatchNorm2d               512\nConv2d                    32,768\nBatchNorm2d               256\nConv2d                    4,608\nBatchNorm2d               256\nConv2d                    32,768\nBatchNorm2d               512\nConv2d                    32,768\nBatchNorm2d               256\nConv2d                    4,608\nBatchNorm2d               256\nConv2d                    32,768\nBatchNorm2d               512\nConv2d                    65,536\nBatchNorm2d               512\nConv2d                    18,432\nBatchNorm2d               512\nConv2d                    131,072\nBatchNorm2d               1,024\nConv2d                    131,072\nBatchNorm2d               1,024\nConv2d                    131,072\nBatchNorm2d               512\nConv2d                    18,432\nBatchNorm2d               512\nConv2d                    131,072\nBatchNorm2d               1,024\nConv2d                    131,072\nBatchNorm2d               512\nConv2d                    18,432\nBatchNorm2d               512\nConv2d                    131,072\nBatchNorm2d               1,024\nConv2d                    131,072\nBatchNorm2d               512\nConv2d                    18,432\nBatchNorm2d               512\nConv2d                    131,072\nBatchNorm2d               1,024\nConv2d                    262,144\nBatchNorm2d               1,024\nConv2d                    73,728\nBatchNorm2d               1,024\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    524,288\nBatchNorm2d               1,024\nConv2d                    73,728\nBatchNorm2d               1,024\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    524,288\nBatchNorm2d               1,024\nConv2d                    73,728\nBatchNorm2d               1,024\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    524,288\nBatchNorm2d               1,024\nConv2d                    73,728\nBatchNorm2d               1,024\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    524,288\nBatchNorm2d               1,024\nConv2d                    73,728\nBatchNorm2d               1,024\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    524,288\nBatchNorm2d               1,024\nConv2d                    73,728\nBatchNorm2d               1,024\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    1,048,576\nBatchNorm2d               2,048\nConv2d                    294,912\nBatchNorm2d               2,048\nConv2d                    2,097,152\nBatchNorm2d               4,096\nConv2d                    2,097,152\nBatchNorm2d               4,096\nConv2d                    2,097,152\nBatchNorm2d               2,048\nConv2d                    294,912\nBatchNorm2d               2,048\nConv2d                    2,097,152\nBatchNorm2d               4,096\nConv2d                    2,097,152\nBatchNorm2d               2,048\nConv2d                    294,912\nBatchNorm2d               2,048\nConv2d                    2,097,152\nBatchNorm2d               4,096\nAdaptiveAvgPool2d         0\nLinear                    20,490\n\nModel Architecture Type:\nResNet\n\nStarting training ResNeXt-50...\nEpoch [1/50], Train Loss: 0.5799, Val Loss: 0.1851, Val Accuracy: 0.9409\nModel saved with validation accuracy: 0.9409\nEpoch [2/50], Train Loss: 0.2356, Val Loss: 0.1403, Val Accuracy: 0.9574\nModel saved with validation accuracy: 0.9574\nEpoch [3/50], Train Loss: 0.1487, Val Loss: 0.1540, Val Accuracy: 0.9505\nEpoch [4/50], Train Loss: 0.1085, Val Loss: 0.1726, Val Accuracy: 0.9532\nEpoch [5/50], Train Loss: 0.0760, Val Loss: 0.1031, Val Accuracy: 0.9739\nModel saved with validation accuracy: 0.9739\nEpoch [6/50], Train Loss: 0.0700, Val Loss: 0.0970, Val Accuracy: 0.9725\nEpoch [7/50], Train Loss: 0.0493, Val Loss: 0.1267, Val Accuracy: 0.9587\nEpoch [8/50], Train Loss: 0.0576, Val Loss: 0.1238, Val Accuracy: 0.9656\nEpoch [9/50], Train Loss: 0.0590, Val Loss: 0.0760, Val Accuracy: 0.9821\nModel saved with validation accuracy: 0.9821\nEpoch [10/50], Train Loss: 0.0788, Val Loss: 0.1343, Val Accuracy: 0.9670\nEpoch [11/50], Train Loss: 0.0503, Val Loss: 0.1413, Val Accuracy: 0.9615\nEpoch [12/50], Train Loss: 0.0368, Val Loss: 0.0921, Val Accuracy: 0.9739\nEpoch [13/50], Train Loss: 0.0477, Val Loss: 0.0648, Val Accuracy: 0.9862\nModel saved with validation accuracy: 0.9862\nEpoch [14/50], Train Loss: 0.0215, Val Loss: 0.0726, Val Accuracy: 0.9752\nEpoch [15/50], Train Loss: 0.0192, Val Loss: 0.0792, Val Accuracy: 0.9849\nEpoch [16/50], Train Loss: 0.0255, Val Loss: 0.1237, Val Accuracy: 0.9766\nEpoch [17/50], Train Loss: 0.0429, Val Loss: 0.1311, Val Accuracy: 0.9656\nEpoch [18/50], Train Loss: 0.0158, Val Loss: 0.0636, Val Accuracy: 0.9780\nEarly stopping triggered after 18 epochs\n\nEvaluating ResNeXt-50 on test set...\n\nResNeXt-50 Classification Report:\n                        precision    recall  f1-score   support\n\n    3_long_blade_rotor       0.98      0.99      0.99       118\n 3_short_blade_rotor_1       0.97      0.94      0.96        71\n 3_short_blade_rotor_2       0.97      0.93      0.95        67\n                  Bird       1.00      1.00      1.00       107\nBird+mini-helicopter_1       0.96      0.89      0.93        57\nBird+mini-helicopter_2       0.94      0.96      0.95        50\n            RC plane_1       0.90      1.00      0.95        56\n            RC plane_2       1.00      1.00      1.00        66\n               drone_1       1.00      0.92      0.96        63\n               drone_2       0.92      1.00      0.96        73\n\n              accuracy                           0.97       728\n             macro avg       0.97      0.96      0.96       728\n          weighted avg       0.97      0.97      0.97       728\n\n\nResNeXt-50 Multi-class ROC AUC Score: 0.9980\n\nResNeXt-50 Total Number of Parameters: 23,000,394\nResNeXt-50 Average Inference Time per Sample: 7.686 ms\nResNeXt-50 FLOPs: 4,286,156,800.0 (4.29 G)\nResNeXt-50 MACs: 23,000,394.0 (23.00 M)\n\n✅ ResNeXt-50 Accuracy for class '3_long_blade_rotor': 99.15%\n✅ ResNeXt-50 Accuracy for class '3_short_blade_rotor_1': 94.37%\n✅ ResNeXt-50 Accuracy for class '3_short_blade_rotor_2': 92.54%\n✅ ResNeXt-50 Accuracy for class 'Bird': 100.00%\n✅ ResNeXt-50 Accuracy for class 'Bird+mini-helicopter_1': 89.47%\n✅ ResNeXt-50 Accuracy for class 'Bird+mini-helicopter_2': 96.00%\n✅ ResNeXt-50 Accuracy for class 'RC plane_1': 100.00%\n✅ ResNeXt-50 Accuracy for class 'RC plane_2': 100.00%\n✅ ResNeXt-50 Accuracy for class 'drone_1': 92.06%\n✅ ResNeXt-50 Accuracy for class 'drone_2': 100.00%\n\n✅ ResNeXt-50 Test Set Accuracy: 0.968\n📊 ResNeXt-50 Model Size: 87.74 MB\n\nKey characteristics of ResNeXt-50:\n- Extension of ResNet that aggregates residual transformations\n- Uses split-transform-merge strategy with grouped convolutions\n- Better performance than ResNet with similar complexity\n- Higher accuracy-to-computation ratio than many models\n- Cardinality dimension provides a more effective way to adjust model capacity\n\nMetrics saved to results/ResNeXt-50/resnext_50_metrics.json\n\n\n================================================================================\nTRAINING WIDE RESNET-50\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\" to /root/.cache/torch/hub/checkpoints/wide_resnet50_2-95faca4d.pth\n100%|██████████| 132M/132M [00:08<00:00, 16.9MB/s] \n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nTraining and Evaluating Wide ResNet-50\n==================================================\n\nWide ResNet-50 Summary:\nModel device: cuda:0\nTotal Parameters: 66,854,730\nTrainable Parameters: 66,854,730\n\nKey Layers:\n--------------------------------------------------------------------------------\nLayer Type                Parameters     \n--------------------------------------------------------------------------------\nConv2d                    9,408\nBatchNorm2d               128\nMaxPool2d                 0\nConv2d                    8,192\nBatchNorm2d               256\nConv2d                    147,456\nBatchNorm2d               256\nConv2d                    32,768\nBatchNorm2d               512\nConv2d                    16,384\nBatchNorm2d               512\nConv2d                    32,768\nBatchNorm2d               256\nConv2d                    147,456\nBatchNorm2d               256\nConv2d                    32,768\nBatchNorm2d               512\nConv2d                    32,768\nBatchNorm2d               256\nConv2d                    147,456\nBatchNorm2d               256\nConv2d                    32,768\nBatchNorm2d               512\nConv2d                    65,536\nBatchNorm2d               512\nConv2d                    589,824\nBatchNorm2d               512\nConv2d                    131,072\nBatchNorm2d               1,024\nConv2d                    131,072\nBatchNorm2d               1,024\nConv2d                    131,072\nBatchNorm2d               512\nConv2d                    589,824\nBatchNorm2d               512\nConv2d                    131,072\nBatchNorm2d               1,024\nConv2d                    131,072\nBatchNorm2d               512\nConv2d                    589,824\nBatchNorm2d               512\nConv2d                    131,072\nBatchNorm2d               1,024\nConv2d                    131,072\nBatchNorm2d               512\nConv2d                    589,824\nBatchNorm2d               512\nConv2d                    131,072\nBatchNorm2d               1,024\nConv2d                    262,144\nBatchNorm2d               1,024\nConv2d                    2,359,296\nBatchNorm2d               1,024\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    524,288\nBatchNorm2d               1,024\nConv2d                    2,359,296\nBatchNorm2d               1,024\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    524,288\nBatchNorm2d               1,024\nConv2d                    2,359,296\nBatchNorm2d               1,024\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    524,288\nBatchNorm2d               1,024\nConv2d                    2,359,296\nBatchNorm2d               1,024\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    524,288\nBatchNorm2d               1,024\nConv2d                    2,359,296\nBatchNorm2d               1,024\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    524,288\nBatchNorm2d               1,024\nConv2d                    2,359,296\nBatchNorm2d               1,024\nConv2d                    524,288\nBatchNorm2d               2,048\nConv2d                    1,048,576\nBatchNorm2d               2,048\nConv2d                    9,437,184\nBatchNorm2d               2,048\nConv2d                    2,097,152\nBatchNorm2d               4,096\nConv2d                    2,097,152\nBatchNorm2d               4,096\nConv2d                    2,097,152\nBatchNorm2d               2,048\nConv2d                    9,437,184\nBatchNorm2d               2,048\nConv2d                    2,097,152\nBatchNorm2d               4,096\nConv2d                    2,097,152\nBatchNorm2d               2,048\nConv2d                    9,437,184\nBatchNorm2d               2,048\nConv2d                    2,097,152\nBatchNorm2d               4,096\nAdaptiveAvgPool2d         0\nLinear                    20,490\n\nModel Architecture Type:\nResNet\n\nStarting training Wide ResNet-50...\nEpoch [1/50], Train Loss: 0.6299, Val Loss: 0.4414, Val Accuracy: 0.8418\nModel saved with validation accuracy: 0.8418\nEpoch [2/50], Train Loss: 0.2787, Val Loss: 0.2129, Val Accuracy: 0.9326\nModel saved with validation accuracy: 0.9326\nEpoch [3/50], Train Loss: 0.1480, Val Loss: 0.1295, Val Accuracy: 0.9642\nModel saved with validation accuracy: 0.9642\nEpoch [4/50], Train Loss: 0.1115, Val Loss: 0.1212, Val Accuracy: 0.9642\nEpoch [5/50], Train Loss: 0.1019, Val Loss: 0.1266, Val Accuracy: 0.9656\nModel saved with validation accuracy: 0.9656\nEpoch [6/50], Train Loss: 0.0633, Val Loss: 0.0683, Val Accuracy: 0.9807\nModel saved with validation accuracy: 0.9807\nEpoch [7/50], Train Loss: 0.0644, Val Loss: 0.0791, Val Accuracy: 0.9752\nEpoch [8/50], Train Loss: 0.0634, Val Loss: 0.0928, Val Accuracy: 0.9656\nEpoch [9/50], Train Loss: 0.0561, Val Loss: 0.0728, Val Accuracy: 0.9807\nEpoch [10/50], Train Loss: 0.0829, Val Loss: 0.0886, Val Accuracy: 0.9739\nEpoch [11/50], Train Loss: 0.0249, Val Loss: 0.0527, Val Accuracy: 0.9862\nModel saved with validation accuracy: 0.9862\nEpoch [12/50], Train Loss: 0.0139, Val Loss: 0.0535, Val Accuracy: 0.9849\nEpoch [13/50], Train Loss: 0.0071, Val Loss: 0.0484, Val Accuracy: 0.9849\nEpoch [14/50], Train Loss: 0.0059, Val Loss: 0.0484, Val Accuracy: 0.9849\nEpoch [15/50], Train Loss: 0.0064, Val Loss: 0.0490, Val Accuracy: 0.9862\nEpoch [16/50], Train Loss: 0.0036, Val Loss: 0.0503, Val Accuracy: 0.9862\nEarly stopping triggered after 16 epochs\n\nEvaluating Wide ResNet-50 on test set...\n\nWide ResNet-50 Classification Report:\n                        precision    recall  f1-score   support\n\n    3_long_blade_rotor       0.97      0.99      0.98       118\n 3_short_blade_rotor_1       0.96      0.97      0.97        71\n 3_short_blade_rotor_2       0.98      0.96      0.97        67\n                  Bird       1.00      1.00      1.00       107\nBird+mini-helicopter_1       0.93      0.89      0.91        57\nBird+mini-helicopter_2       0.94      0.92      0.93        50\n            RC plane_1       0.97      1.00      0.98        56\n            RC plane_2       1.00      1.00      1.00        66\n               drone_1       1.00      0.95      0.98        63\n               drone_2       0.96      1.00      0.98        73\n\n              accuracy                           0.97       728\n             macro avg       0.97      0.97      0.97       728\n          weighted avg       0.97      0.97      0.97       728\n\n\nWide ResNet-50 Multi-class ROC AUC Score: 0.9995\n\nWide ResNet-50 Total Number of Parameters: 66,854,730\nWide ResNet-50 Average Inference Time per Sample: 7.875 ms\nWide ResNet-50 FLOPs: 11,453,698,048.0 (11.45 G)\nWide ResNet-50 MACs: 66,854,730.0 (66.85 M)\n\n✅ Wide ResNet-50 Accuracy for class '3_long_blade_rotor': 99.15%\n✅ Wide ResNet-50 Accuracy for class '3_short_blade_rotor_1': 97.18%\n✅ Wide ResNet-50 Accuracy for class '3_short_blade_rotor_2': 95.52%\n✅ Wide ResNet-50 Accuracy for class 'Bird': 100.00%\n✅ Wide ResNet-50 Accuracy for class 'Bird+mini-helicopter_1': 89.47%\n✅ Wide ResNet-50 Accuracy for class 'Bird+mini-helicopter_2': 92.00%\n✅ Wide ResNet-50 Accuracy for class 'RC plane_1': 100.00%\n✅ Wide ResNet-50 Accuracy for class 'RC plane_2': 100.00%\n✅ Wide ResNet-50 Accuracy for class 'drone_1': 95.24%\n✅ Wide ResNet-50 Accuracy for class 'drone_2': 100.00%\n\n✅ Wide ResNet-50 Test Set Accuracy: 0.974\n📊 Wide ResNet-50 Model Size: 255.03 MB\n\nKey characteristics of Wide ResNet-50:\n- Modification of ResNet with increased width (channel count) and reduced depth\n- Wider networks often achieve better performance than deeper ones\n- More efficient to train than very deep networks\n- Better feature extraction capability with wider channels\n- Strong classification performance with reasonable computational cost\n\nMetrics saved to results/Wide ResNet-50/wide resnet_50_metrics.json\n\n\nAll models have been trained and evaluated!\nResults have been saved to individual folders in the 'results' directory\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}